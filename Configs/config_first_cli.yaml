seed_everything: 23
# ckpt_path : checkpoints/2025-01-08T02-39-00/last.ckpt
wandb: False
debug: False


trainer:
  benchmark: True
  gradient_clip_algorithm: norm
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  devices: 2
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  log_every_n_steps: 10
  # check_val_every_n_epoch: 1
  val_check_interval: 0.25
  # enable_checkpointing: True
  # num_sanity_val_steps: 0
  max_steps: 100000
  profiler: advanced
  # precision: bf16

  callbacks:
    - class_path: d3rmsq.callbacks.ema.EMACallback
      init_args:
        decay: 0.99
        use_ema_weights: True
        update_interval: 25
    - class_path: d3rmsq.callbacks.train_speed.TrainingSpeedCallback
      init_args:
        log_every_n_steps: 10

model:
  pretrained_model: ""
  second_stage_load_pretrained: true

  content_head:
  prompt_head:

  decoder:  
    class_path: d3rmsq.d3rm.decoder.NATDecoder
    init_args:
      label_embed_dim: 16
      dim: 256
      content_dim: 1
      prompt_dim: 1
      n_layers: 10
      # window: [5,5,5,5,5,5,5,5,5,5]
      window: [31,31,31,31,31,31,31,31,31,31]
      # window: [31,31,31,31,31,31,31,31,31,31]
      dilation: [1,1,1,1,1,1,1,1,1,1]
      condition_method: cross
      diffusion_step: 100
      timestep_type: adalayernorm
      natten_direction: 2d
      num_state: *num_state
      classifier_free_guidance: False
      nat_bias: True
      nat_dropout: 0.0 # increases GPU memory
      use_lstm: False
      lstm_layers: 0
      use_fna: False
    
  codec:
    class_path: d3rmsq.sqcodec.scalar16k.ScalarModel
    init_args:
      num_bands: 1
      sample_rate: 16000
      causal: true
      num_samples: 2
      downsample_factors: [2, 4, 4, 5]
      downsample_kernel_sizes: [4, 8, 8, 10]
      upsample_factors: [5, 4, 4, 2]
      upsample_kernel_sizes: [10, 8, 8, 4]
      latent_hidden_dim: 32
      default_kernel_size: 7
      delay_kernel_size: 5
      init_channel: 64
      res_kernel_size: 7
      inference: True

  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 2.5e-4
      weight_decay: 4.5e-2
      betas: [0.9, 0.96]

  scheduler:
    monitor: train/diffusion_loss
    factor: 0.8
    patience: 25000
    min_lr: 1.0e-5
    threshold: 1.0e-1
    threshold_mode: rel
    # warmup_lr: 4.5e-4
  # warmup: 1000


data:
  wav_scp: ./data/train_all/wav.scp
  vqidx_scp: ./feats/vqidx/train_all/feats.scp
  mel_scp: ./feats/normed_fbank/train_all/feats.scp
  prompt_scp: ./feats/wavlm_l6/train_all/feats.scp
  utt2num_frames: ./data/train_all/utt2num_frames
  # segments: null
  batch_frames: 7200 # 3600
  batch_size: null
  min_num_frames: 600 # 600 * 160 / 16000 = 6s
  max_num_frames: 3000 # 30s
  allow_cache: False
  prompt_fold_by_2: True
  test_prompt_duration: 3.0
  test_num_samples: 10
  # train: True
  # validation: True
  # test: True
  # predict: False
  prompt_dim: *prompt_dim
  hop_size: *hop_size
  win_length: *win_length
  sampling_rate: *sr
  test_list_path: ./results/LibriTTS/